import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ====== 1. 引入 CLIP 关键词模块 ======
from clip_keywords import get_clip_keywords_from_video


# ====== 2. Qwen 本地模型路径（B 方案） ======
QWEN_MODEL_DIR = r"D:\pythondata\biye1\models\qwen2.5\models--Qwen--Qwen2.5-1.5B-Instruct\snapshots\989aa7980e4cf806f80c7fef2b1adb7bc71aa306"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")


# ====== 3. 加载 Qwen ======
print("Loading Qwen tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    QWEN_MODEL_DIR,
    trust_remote_code=True
)

print("Loading Qwen model...")
model = AutoModelForCausalLM.from_pretrained(
    QWEN_MODEL_DIR,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
    device_map="auto",
    trust_remote_code=True
)
model.eval()

print("Qwen model loaded successfully!")
model = model.float()

print("Model device:", next(model.parameters()).device)
if torch.cuda.is_available():
    print("Before generation:")
    print("Allocated:",
          torch.cuda.memory_allocated() / 1024**2, "MB")



# ====== 4. 构造 Prompt（关键词 + 用户问题） ======
def build_prompt(clip_keywords, user_question):
    keywords_text = ", ".join(clip_keywords)

    prompt = f"""
You are a video question answering assistant.

Video keywords: {keywords_text}

Question: {user_question}

Give a concise factual answer.
If the answer is obvious, answer directly.
If unsure, give the most likely answer.
"""
    return prompt.strip()


# ====== 5. 视频问答主函数 ======
def video_qa(video_path, user_question):
    # ① CLIP：视频 → 关键词
    clip_keywords = get_clip_keywords_from_video(video_path)

    print("\nCLIP 自动关键词:")
    for k in clip_keywords:
        print(f"- {k}")

    # ② 构造 Prompt
    prompt = build_prompt(clip_keywords, user_question)

    messages = [
        {"role": "system", "content": "You are Qwen, a helpful multimodal assistant."},
        {"role": "user", "content": prompt}
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    print(text)

    inputs = tokenizer(
        text,
        return_tensors="pt"
    ).to(model.device)

    if "inputs_embeds" in inputs:
        print("inputs_embeds finite:",
              torch.isfinite(inputs["inputs_embeds"]).all())

    for k, v in inputs.items():
        if torch.is_tensor(v):
            print(
                k,
                v.dtype,
                torch.isfinite(v).all().item(),
                v.min().item(),
                v.max().item()
            )

    with torch.no_grad():
        out = model(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"]
        )
        logits = out.logits
        print(
            "logits finite:",
            torch.isfinite(logits).all().item(),
            "min:", logits.min().item(),
            "max:", logits.max().item()
        )

    # ③ Qwen 推理
    outputs = model.generate(
        **inputs,
        max_new_tokens=60,
        do_sample=True,  # ✅ 关键
        temperature=0.7,
        top_p=0.9,
        top_k=50,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id
    )
    output_ids = outputs[0][inputs["input_ids"].shape[1]:]
    answer = tokenizer.decode(output_ids, skip_special_tokens=True)

    return answer.strip()


# ====== 6. CLI 入口 ======
if __name__ == "__main__":
    video_path = input("请输入视频路径: ").strip()
    question = input("请输入你的问题: ").strip()

    print("\n=== 系统回答 ===")
    answer = video_qa(video_path, question)
    print(answer)
